{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Vectorisation\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is vectorisation?\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Python is not the fastest programming language. So when you need to process a large amount of homogeneous data quickly, you’re told to rely on **vectorization**.\n",
    "\n",
    "- Let’s say we have a few million numbers in a list or array, and we want to do some mathematical operations on them. Since we know they are all numbers, and if we’re doing the same operation on all of the numbers, we can **vectorise** the operation, i.e. take advantage of this **homogeneity** of data and operation.\n",
    "    \n",
    "- In python this means that a batch operation implemented in a fast language: say C.\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T16:33:15.634292Z",
     "start_time": "2022-01-21T16:33:15.629650Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from subprocess import Popen\n",
    "from os import getpid\n",
    "from signal import SIGINT\n",
    "from time import sleep, time\n",
    "from resource import getrusage, RUSAGE_SELF\n",
    "\n",
    "events = [\n",
    "    \"instructions\",\n",
    "    \"cache-references\",\n",
    "    \"cache-misses\",\n",
    "    \"avx_insts.all\",\n",
    "]\n",
    "\n",
    "@contextmanager\n",
    "def perf():\n",
    "    \"\"\"Benchmark this process with Linux's perf util.\n",
    "    \n",
    "    Example usage:\n",
    "\n",
    "        with perf():\n",
    "            x = run_some_code()\n",
    "            more_code(x)\n",
    "            all_this_code_will_be_measured()\n",
    "    \"\"\"\n",
    "    p = Popen([\n",
    "            # Run perf stat\n",
    "            \"perf\", \"stat\",\n",
    "            # for the current Python process\n",
    "            \"-p\", str(getpid()),\n",
    "            # record the list of events mentioned above\n",
    "            \"-e\", \",\".join(events)])\n",
    "    # Ensure perf has started before running more\n",
    "    # Python code. This will add ~0.1 to the elapsed\n",
    "    # time reported by perf, so we also track elapsed\n",
    "    # time separately.\n",
    "    sleep(0.1)\n",
    "    start = time()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        print(f\"Elapsed (seconds): {time() - start}\")\n",
    "        print(\"Peak memory (MiB):\",\n",
    "            int(getrusage(RUSAGE_SELF).ru_maxrss / 1024))\n",
    "        p.send_signal(SIGINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T16:30:07.054046Z",
     "start_time": "2022-01-21T16:30:04.778387Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "DATA = [random() for _ in range(30_000_000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with perf():    \n",
    "    mean = sum(DATA) / len(DATA)\n",
    "    result = [DATA[i] - mean for i in range(len(DATA))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- https://pythonspeed.com/articles/vectorization-python/\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "343.722px",
    "left": "1262.27px",
    "right": "20px",
    "top": "120px",
    "width": "365px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
