{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8b54dc",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6685401e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T13:35:39.761234Z",
     "start_time": "2023-06-21T13:35:39.053632Z"
    },
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506d474",
   "metadata": {},
   "source": [
    "## Types of Parallelization\n",
    "\n",
    "Large textbooks have been written on different approaches to parallelization but we will keep a tight focus on what’s most useful to us.\n",
    "\n",
    "We will briefly review the two main kinds of parallelization commonly used in\n",
    "scientific computing and discuss their pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37bd9b",
   "metadata": {},
   "source": [
    "### Multiprocessing\n",
    "\n",
    "Multiprocessing means concurrent execution of multiple processes using more than one processor.\n",
    "\n",
    "In this context, a **process** is a chain of instructions (i.e., a program).\n",
    "\n",
    "Multiprocessing can be carried out on one machine with multiple CPUs or on a\n",
    "collection of machines connected by a network.\n",
    "\n",
    "In the latter case, the collection of machines is usually called a\n",
    "**cluster**.\n",
    "\n",
    "With multiprocessing, each process has its own memory space, although the\n",
    "physical memory chip might be shared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795c854",
   "metadata": {},
   "source": [
    "### Multithreading\n",
    "\n",
    "Multithreading is similar to multiprocessing, except that, during execution, the threads all share the same memory space.\n",
    "\n",
    "Native Python struggles to implement multithreading due to some [legacy design\n",
    "features](https://wiki.python.org/moin/GlobalInterpreterLock).\n",
    "\n",
    "But this is not a restriction for scientific libraries like NumPy and Numba.\n",
    "\n",
    "Functions imported from these libraries and JIT-compiled code run in low level\n",
    "execution environments where Python’s legacy restrictions don’t apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da658d27",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "\n",
    "Multithreading is more lightweight because most system and memory resources\n",
    "are shared by the threads.\n",
    "\n",
    "In addition, the fact that multiple threads all access a shared pool of memory\n",
    "is extremely convenient for numerical programming.\n",
    "\n",
    "On the other hand, multiprocessing is more flexible and can be distributed\n",
    "across clusters.\n",
    "\n",
    "For the great majority of what we do in these lectures, multithreading will\n",
    "suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce06ec",
   "metadata": {},
   "source": [
    "## Implicit Multithreading in NumPy\n",
    "\n",
    "Actually, you have already been using multithreading in your Python code,\n",
    "although you might not have realized it.\n",
    "\n",
    "(We are, as usual, assuming that you are running the latest version of\n",
    "Anaconda Python.)\n",
    "\n",
    "This is because NumPy cleverly implements multithreading in a lot of its\n",
    "compiled code.\n",
    "\n",
    "Let’s look at some examples to see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c30739",
   "metadata": {},
   "source": [
    "### A Matrix Operation\n",
    "\n",
    "- The next piece of code computes the eigenvalues of a large number of randomly\n",
    "generated matrices.\n",
    "- It takes a few seconds to run.\n",
    "- you can check this by typing `htop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49236c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T13:35:50.680242Z",
     "start_time": "2023-06-21T13:35:39.762864Z"
    },
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "n = 20\n",
    "m = 1000\n",
    "for i in range(n):\n",
    "    X = np.random.randn(m, m)\n",
    "    λ = np.linalg.eigvals(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7a193",
   "metadata": {},
   "source": [
    "This is because NumPy’s `eigvals` routine neatly splits up the tasks and\n",
    "distributes them to different threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865bc4a",
   "metadata": {},
   "source": [
    "### A Comparison with Numba\n",
    "\n",
    "To get some basis for comparison for the last example, let’s try the same\n",
    "thing with Numba.\n",
    "\n",
    "In fact there is an easy way to do this, since Numba can also be used to\n",
    "create custom [ufuncs](https://python-programming.quantecon.org/need_for_speed.html#ufuncs) with the [@vectorize](http://numba.pydata.org/numba-doc/dev/user/vectorize.html) decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698e4039",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T13:35:51.819760Z",
     "start_time": "2023-06-21T13:35:50.683233Z"
    },
    "hide-output": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999992797121728"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import vectorize\n",
    "\n",
    "@vectorize\n",
    "def f_vec(x, y):\n",
    "    return np.cos(x**2 + y**2) / (1 + x**2 + y**2)\n",
    "\n",
    "grid = np.linspace(-3,3,5000)\n",
    "x,y = np.meshgrid(grid, grid)\n",
    "np.max(f_vec(x, y))  # Run once to compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe46729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T13:35:54.456903Z",
     "start_time": "2023-06-21T13:35:51.821616Z"
    },
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 ms ± 13.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.max(f_vec(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89492964",
   "metadata": {},
   "source": [
    "At least on our machine, the difference in the speed between the\n",
    "Numba version and the vectorized NumPy version shown above is not large.\n",
    "\n",
    "But there’s quite a bit going on here so let’s try to break down what is\n",
    "happening.\n",
    "\n",
    "Both Numba and NumPy use efficient machine code that’s specialized to these\n",
    "floating point operations.\n",
    "\n",
    "However, the code NumPy uses is, in some ways, less efficient.\n",
    "\n",
    "The reason is that, in NumPy, the operation `np.cos(x**2 + y**2) / (1 + x**2 + y**2)` generates several intermediate arrays.\n",
    "\n",
    "For example, a new array is created when `x**2` is calculated.\n",
    "\n",
    "The same is true when `y**2` is calculated, and then `x**2 + y**2` and so on.\n",
    "\n",
    "Numba avoids creating all these intermediate arrays by compiling one\n",
    "function that is specialized to the entire operation.\n",
    "\n",
    "But if this is true, then why isn’t the Numba code faster?\n",
    "\n",
    "The reason is that NumPy makes up for its disadvantages with implicit\n",
    "multithreading, as we’ve just discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32c312",
   "metadata": {},
   "source": [
    "### Multithreading a Numba Ufunc\n",
    "\n",
    "Can we get both of these advantages at once?\n",
    "\n",
    "In other words, can we pair\n",
    "\n",
    "- the efficiency of Numba’s highly specialized JIT compiled function and  \n",
    "- the speed gains from parallelization obtained by NumPy’s implicit\n",
    "  multithreading?  \n",
    "\n",
    "\n",
    "It turns out that we can, by adding some type information plus `target='parallel'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7794687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T13:35:55.331300Z",
     "start_time": "2023-06-21T13:35:54.459008Z"
    },
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999992797121728"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@vectorize('float64(float64, float64)', target='parallel')\n",
    "def f_vec(x, y):\n",
    "    return np.cos(x**2 + y**2) / (1 + x**2 + y**2)\n",
    "\n",
    "np.max(f_vec(x, y))  # Run once to compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f3ba33c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T13:36:01.060571Z",
     "start_time": "2023-06-21T13:35:55.333196Z"
    },
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711 ms ± 54.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.max(f_vec(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b20a3a",
   "metadata": {},
   "source": [
    "Now our code runs significantly faster than the NumPy version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2956",
   "metadata": {},
   "source": [
    "- https://github.com/QuantEcon/lecture-python-programming.notebooks/blob/master/parallelization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232774f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "date": 1684407286.3284774,
  "filename": "parallelization.md",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "title": "Parallelization",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
